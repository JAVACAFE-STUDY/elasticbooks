#분석기 테스트
POST /wiki_analyzer/_analyze
{
  "tokenizer": "letter",
  "text": "The Elasticsearch, is_good"
}


{
  "tokens": [
    {
      "token": "The",
      "start_offset": 0,
      "end_offset": 3,
      "type": "word",
      "position": 0
    },
    {
      "token": "Elasticsearch",
      "start_offset": 4,
      "end_offset": 17,
      "type": "word",
      "position": 1
    },
    {
      "token": "is",
      "start_offset": 19,
      "end_offset": 21,
      "type": "word",
      "position": 2
    },
    {
      "token": "good",
      "start_offset": 22,
      "end_offset": 26,
      "type": "word",
      "position": 3
    }
  ]
}